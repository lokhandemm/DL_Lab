# Single-Layer Perceptron for Logic Gates

## ðŸ“Œ Overview
This project implements a **Single-Layer Perceptron** to classify binary inputs using a **Step Activation Function**. It is used to simulate basic logic gates (**AND, OR, NOT**) and extended with a **Multi-Layer Perceptron (MLP)** to handle **XOR and XNOR**, which are non-linearly separable.

## ðŸš€ Implementation
- A **Single-Layer Perceptron** is used for basic gates like **AND**.
- A **Multi-Layer Perceptron (MLP)** is used for **XOR and XNOR**.
- The model is tested and validated with all binary input combinations.

## ðŸ“Š Truth Table Validation

| Input (X1, X2) | AND Output | XOR Output | XNOR Output |
|---------------|------------|------------|------------|
| (0,0)        | 0          | 0          | 1          |
| (0,1)        | 0          | 1          | 0          |
| (1,0)        | 0          | 1          | 0          |
| (1,1)        | 1          | 0          | 1          |


