{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AIM:**: Implement the perceptron learning algorithm to classify 3-dimensional points into two classes based on a given dataset. Update weights iteratively until convergence.\n",
        ": Develop a Python program to determine the number of linearly separable binary classification problems from a given dataset of 3-dimensional points. Validate using multiple class combinations.\n",
        ": Analyze the convergence of the perceptron learning algorithm by varying the learning rate and epochs. Evaluate its impact on classification accuracy."
      ],
      "metadata": {
        "id": "KKJ-1UqsJSwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data_points={\n",
        "    '0': np.array([-1, -1, -1]),\n",
        "    '1': np.array([-1,-1,1]),\n",
        "    '2': np.array([-1, 1, -1]),\n",
        "    '3': np.array([-1, 1, 1]),\n",
        "    '4': np.array([1, -1, -1]),\n",
        "    '5': np.array([1, -1, 1]),\n",
        "    '6': np.array([1, 1, -1]),\n",
        "    '7': np.array([1, 1, 1]),\n",
        "}\n",
        "c1=['7']\n",
        "c2=['0','1','2','3','4','5','6']\n",
        "def step(input_sum,threshold):\n",
        "  if input_sum>threshold:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "def perceptron(inputs,weights,bias):\n",
        "  input_sum=np.dot(inputs,weights)+bias\n",
        "  return step(input_sum, 0)\n",
        "def train_perceptron(data_points,c1,c2,learning_rate=0.5):\n",
        "  weights=np.array([0,0,0],dtype=float)\n",
        "  bias=0.0\n",
        "  while True:\n",
        "    misclassified_count=0\n",
        "    for point_name,point_data in data_points.items():\n",
        "      expected_output=0 if point_name in c1 else 1\n",
        "      actual_output =perceptron(point_data,weights,bias)\n",
        "      if actual_output!= expected_output:\n",
        "        misclassified_count+=1\n",
        "        weights =weights+learning_rate *(expected_output - actual_output)*point_data\n",
        "        bias=bias+learning_rate*(expected_output - actual_output)\n",
        "    if misclassified_count == 0:\n",
        "      break\n",
        "  return weights,bias\n",
        "weights, bias = train_perceptron(data_points, c1, c2)\n",
        "print(\"Optimized wt:\",weights)\n",
        "print(\"Optimized bias:\",bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlaEYImjC7IO",
        "outputId": "aacbd056-88d6-44de-8d9b-88e94069fe42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized wt: [-0.5 -0.5 -0.5]\n",
            "Optimized bias: 1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nand gate\n",
        "def step(input_sum,threshold):\n",
        "  if(input_sum >threshold):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "def nand_per(inputs,weight):\n",
        "  inputs_sum=0\n",
        "  threshold=0\n",
        "  bias=2.5\n",
        "  for i in range(len(inputs)):\n",
        "    inputs_sum+=inputs[i]*weight[i]\n",
        "  return step(inputs_sum+bias,threshold)\n",
        "inputs=[[-1,-1,-1],[-1,-1,1],[-1,1,-1],[-1,1,1],[1,-1,-1],[1,-1,1],[1,1,-1],[1,1,1]]\n",
        "for i in inputs:\n",
        "  weight=[-0.5,-0.5,-0.5]\n",
        "  print(i,\"=\",nand_per(i,weight))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6ytS8D0D0rX",
        "outputId": "c3913238-e349-423b-da65-a7c0d7c91b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1, -1, -1] = 1\n",
            "[-1, -1, 1] = 1\n",
            "[-1, 1, -1] = 1\n",
            "[-1, 1, 1] = 1\n",
            "[1, -1, -1] = 1\n",
            "[1, -1, 1] = 1\n",
            "[1, 1, -1] = 1\n",
            "[1, 1, 1] = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "points = np.array([[-1, -1, -1],\n",
        "                   [-1, -1, 1],\n",
        "                   [-1, 1, -1],\n",
        "                   [-1, 1, 1],\n",
        "                   [1, -1, -1],\n",
        "                   [1, -1, 1],\n",
        "                   [1, 1, -1],\n",
        "                   [1, 1, 1]])\n",
        "def is_linearly_separable(labels):\n",
        "    clf=SVC(kernel='linear')\n",
        "    try:\n",
        "        clf.fit(points,labels)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "labelings=product([0, 1],repeat=8)\n",
        "separable_count=0\n",
        "for labeling in labelings:\n",
        "    if is_linearly_separable(np.array(labeling)):\n",
        "        separable_count=separable_count+ 1\n",
        "print(f\"No of linearly separable:{separable_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsjuL2wUEGGo",
        "outputId": "3d7d8846-04f5-4694-c432-d3880ff4801a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of linearly separable:254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CONCLUSION:**"
      ],
      "metadata": {
        "id": "cuGYzYrYI5rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perceptron successfully learns to classify the given data by adjusting weights and bias using different learning rates. The convergence analysis shows that higher learning rates may lead to faster convergence, but can also cause instability. Lower learning rates provide smoother learning but may require more epochs. Overall, choosing an optimal learning rate is crucial for effective training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zZtHGn3iJAUD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "id3Ugz0fJCfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}